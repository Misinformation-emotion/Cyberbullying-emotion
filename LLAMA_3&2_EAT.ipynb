{"cells":[{"cell_type":"markdown","source":["#Setting"],"metadata":{"id":"gPpjY8lNBaP1"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179079,"status":"ok","timestamp":1717584233467,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"},"user_tz":-60},"id":"QzSvk9-psdeH","outputId":"f58df156-c637-4e55-aa0d-f66154e3ba15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.2.2\n","  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (4.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Collecting triton==2.2.0 (from torch==2.2.2)\n","  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.3.0\n","    Uninstalling triton-2.3.0:\n","      Successfully uninstalled triton-2.3.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.0+cu121\n","    Uninstalling torch-2.3.0+cu121:\n","      Successfully uninstalled torch-2.3.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n","torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\n","torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n","Collecting transformers==4.40.0\n","  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets==2.18.0\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate==0.29.3\n","  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate==0.4.1\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes==0.43.1\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface_hub==0.22.2\n","  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl==0.8.6\n","  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft==0.10.0\n","  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets==2.18.0)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (2.0.3)\n","Collecting xxhash (from datasets==2.18.0)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets==2.18.0)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (3.9.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (2.2.2)\n","Collecting responses<0.19 (from evaluate==0.4.1)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.22.2) (4.12.0)\n","Collecting tyro>=0.5.11 (from trl==0.8.6)\n","  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.29.3) (12.5.40)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.1)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n","  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n","Installing collected packages: xxhash, shtab, dill, responses, multiprocess, huggingface_hub, tyro, transformers, datasets, bitsandbytes, accelerate, trl, peft, evaluate\n","  Attempting uninstall: huggingface_hub\n","    Found existing installation: huggingface-hub 0.23.2\n","    Uninstalling huggingface-hub-0.23.2:\n","      Successfully uninstalled huggingface-hub-0.23.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.1\n","    Uninstalling transformers-4.41.1:\n","      Successfully uninstalled transformers-4.41.1\n","Successfully installed accelerate-0.29.3 bitsandbytes-0.43.1 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 huggingface_hub-0.22.2 multiprocess-0.70.16 peft-0.10.0 responses-0.18.0 shtab-1.7.1 transformers-4.40.0 trl-0.8.6 tyro-0.8.4 xxhash-3.4.1\n"]}],"source":["%pip install \"torch==2.2.2\" tensorboard\n","%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\""]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55274,"status":"ok","timestamp":1717584349588,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"},"user_tz":-60},"id":"N8157Tsw3Vo3","outputId":"e4a45529-8f3c-4079-bee4-3d15c1ed1680"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"h5NPLc7isjdM","executionInfo":{"status":"ok","timestamp":1717584365895,"user_tz":-60,"elapsed":14456,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"outputs":[],"source":["import os\n","import random\n","import functools\n","import csv\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import evaluate\n","from sklearn.metrics import classification_report\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n","\n","from datasets import Dataset, DatasetDict\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorWithPadding\n",")\n"]},{"cell_type":"markdown","source":["#Prepare Emotion data"],"metadata":{"id":"gP6ZTv7zCEe_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGxo4CzOHSKT"},"outputs":[],"source":["filepath=\"./data/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXFnbzq-HOoK"},"outputs":[],"source":["train = pd.read_csv(filepath+\"train.csv\")\n","test = pd.read_csv(filepath+\"test.csv\")\n","go_emotion = pd.read_csv(filepath+\"goemotion.csv\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"eGELKtfFeKdT","executionInfo":{"status":"ok","timestamp":1717584408966,"user_tz":-60,"elapsed":235,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"outputs":[],"source":["neg_emotion=['gratitude','joy']\n","pos_emotion=['anger','disgust']\n","fake_emotion=['surprise']"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"NQfZPdgjeQAD","executionInfo":{"status":"ok","timestamp":1717584407549,"user_tz":-60,"elapsed":4,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"outputs":[],"source":["def emotion_training_data(Pos_num,Neg_num,Fake_num):\n","  emotions_positive=[]\n","  emotions_negative=[]\n","  emotions_fake=[]\n","  for i in range(len(go_emotion)):\n","    for e in pos_emotion:\n","     if go_emotion.iloc[i][e]==1:\n","        emotions_positive.append(go_emotion.text[i])\n","    for e in neg_emotion:\n","     if go_emotion.iloc[i][e]==1:\n","        emotions_negative.append(go_emotion.text[i])\n","    for e in fake_emotion:\n","     if go_emotion.iloc[i][e]==1:\n","        emotions_fake.append(go_emotion.text[i])\n","  #take away multi-labels from original file\n","  same=[e for e in emotions_negative if e  in emotions_positive or e in emotions_fake]\n","  emotions_negative_new=[e for e in emotions_negative if e not in same]\n","  emotions_positive_new=[e for e in emotions_positive if e not in same]\n","  emotions_fake_new=[e for e in emotions_fake if e not in same]\n","  emotion_contents=emotions_positive_new[:Pos_num]+list(emotions_negative_new[:Neg_num])+list(emotions_fake_new[:Fake_num])\n","  emotions_label=[1]*Pos_num+[0]*Neg_num+[2]*Fake_num\n","  return emotion_contents,emotions_label"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"q84kIk33edI6","colab":{"base_uri":"https://localhost:8080/","height":348},"executionInfo":{"status":"error","timestamp":1717584412558,"user_tz":-60,"elapsed":285,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}},"outputId":"a82ad61a-a046-4543-de24-057e893d0383"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'go_emotion' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-95855dd816c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memotion_contents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotions_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memotion_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_emotion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotions_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_emotion_shuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_emotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_emotion = pd.DataFrame(list(zip(emotion_contents, emotions_label)),\n","\u001b[0;32m<ipython-input-5-66566e76344c>\u001b[0m in \u001b[0;36memotion_training_data\u001b[0;34m(Pos_num, Neg_num, Fake_num)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0memotions_negative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0memotions_fake\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo_emotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_emotion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mgo_emotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'go_emotion' is not defined"]}],"source":["emotion_contents,emotions_label=emotion_training_data(2000,1500,200)\n","train_emotion = pd.DataFrame(emotion_contents)\n","train_emotion['label'] = emotions_label\n","train_emotion_shuffle=train_emotion.sample(frac=1)\n","train_emotion = pd.DataFrame(list(zip(emotion_contents, emotions_label)),\n","               columns =['Comments', 'label'])\n","train_emotion_shuffle=train_emotion.sample(frac=1)\n","train_emotion_shuffle.head()"]},{"cell_type":"markdown","source":["#Define main function"],"metadata":{"id":"eHjGXgDnLSD9"}},{"cell_type":"code","source":["def get_performance_metrics(df_test):\n","  y_test = df_test.label\n","  y_pred = df_test.predictions\n","\n","  print(\"Confusion Matrix:\")\n","  print(confusion_matrix(y_test, y_pred))\n","\n","  print(\"\\nClassification Report:\")\n","  print(classification_report(y_test, y_pred))\n","\n","  print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n","  print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n","\n","def llama_preprocessing_function(examples):\n","    return tokenizer(examples['Comments'], truncation=True, max_length=MAX_LEN)"],"metadata":{"id":"SgGB38dsGvtF","executionInfo":{"status":"ok","timestamp":1717584442469,"user_tz":-60,"elapsed":9,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#define which metrics to compute for evaluation\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}"],"metadata":{"id":"Z23zbCOFLaWU","executionInfo":{"status":"ok","timestamp":1717584443500,"user_tz":-60,"elapsed":2,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#define custom trainer\n","class CustomTrainer(Trainer):\n","    def __init__(self, *args, class_weights=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        # Ensure label_weights is a tensor\n","        if class_weights is not None:\n","            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n","        else:\n","            self.class_weights = None\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # Extract labels and convert them to long type for cross_entropy\n","        labels = inputs.pop(\"labels\").long()\n","\n","        # Forward pass\n","        outputs = model(**inputs)\n","\n","        # Extract logits assuming they are directly outputted by the model\n","        logits = outputs.get('logits')\n","\n","        # Compute custom loss with class weights for imbalanced data handling\n","        if self.class_weights is not None:\n","            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n","        else:\n","            loss = F.cross_entropy(logits, labels)\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","def make_predictions(model,df_test):\n","\n","\n","  # Convert summaries to a list\n","  sentences = df_test.Comments.tolist()\n","\n","  # Define the batch size\n","  batch_size = 32  # You can adjust this based on your system's memory capacity\n","\n","  # Initialize an empty list to store the model outputs\n","  all_outputs = []\n","\n","  # Process the sentences in batches\n","  for i in range(0, len(sentences), batch_size):\n","      # Get the batch of sentences\n","      batch_sentences = sentences[i:i + batch_size]\n","\n","      # Tokenize the batch\n","      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","\n","      # Move tensors to the device where the model is (e.g., GPU or CPU)\n","      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n","\n","      # Perform inference and store the logits\n","      with torch.no_grad():\n","          outputs = model(**inputs)\n","          all_outputs.append(outputs['logits'])\n","  final_outputs = torch.cat(all_outputs, dim=0)\n","  df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n","  #df_test['predictions']=df_test['predictions'].apply(lambda l:category_map[l])\n"],"metadata":{"id":"qTagYtGHIiLc","executionInfo":{"status":"ok","timestamp":1717584444684,"user_tz":-60,"elapsed":247,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#Define Model&Training"],"metadata":{"id":"QPxb783vT2am"}},{"cell_type":"code","source":["#model_name = \"meta-llama/Meta-Llama-3-8B\"\n","model_name= \"meta-llama/Llama-2-7b-hf\""],"metadata":{"id":"GZXRaycHEf-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def define_model(model_name):\n","  quantization_config = BitsAndBytesConfig(\n","    load_in_4bit = True, # enable 4-bit quantization\n","    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n","    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n","    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n","  )\n","  lora_config = LoraConfig(\n","    r = 16, # the dimension of the low-rank matrices\n","    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n","    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n","    lora_dropout = 0.05, # dropout probability of the LoRA layers\n","    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n","    task_type = 'SEQ_CLS'\n","  )\n","  model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    quantization_config=quantization_config,\n","    num_labels=3\n","  )\n","  #prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n","  model = prepare_model_for_kbit_training(model)\n","  #get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with\n","  model = get_peft_model(model, lora_config)\n","\n","  #Load tokennizer\n","  tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","  #Update some model configs\n","  model.config.pad_token_id = tokenizer.pad_token_id\n","  model.config.use_cache = False\n","  model.config.pretraining_tp = 1\n","\n","  MAX_LEN = 512\n","  training_args = TrainingArguments(\n","    output_dir = 'emotion_cyberbullying',\n","    learning_rate = 1e-4,\n","    per_device_train_batch_size = 8,\n","    per_device_eval_batch_size = 8,\n","    num_train_epochs = 2,\n","    weight_decay = 0.01,\n","    evaluation_strategy = 'epoch',\n","    save_strategy = 'epoch',\n","    load_best_model_at_end = True)\n","  return model, tokenizer, training_args"],"metadata":{"id":"gQQlW04-FC7l","executionInfo":{"status":"ok","timestamp":1717584449119,"user_tz":-60,"elapsed":336,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def training(dataset,train_class,model,tokenizer,training_args):\n","  class_weights=(1/train_class.label.value_counts(normalize=True).sort_index()).tolist()\n","  class_weights=torch.tensor(class_weights)\n","  class_weights=class_weights/class_weights.sum()\n","\n","  tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n","  tokenized_datasets.set_format(\"torch\")\n","  collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","  trainer = CustomTrainer(\n","      model = model,\n","      args = training_args,\n","      train_dataset = tokenized_datasets['train'],\n","      eval_dataset = tokenized_datasets['val'],\n","      tokenizer = tokenizer,\n","      data_collator = collate_fn,\n","      compute_metrics = compute_metrics,\n","      class_weights=class_weights,\n","  )\n","  train_result = trainer.train()\n","\n","  return model, trainer,train_result\n"],"metadata":{"id":"fCjEjyp8ggOL","executionInfo":{"status":"ok","timestamp":1717584451658,"user_tz":-60,"elapsed":224,"user":{"displayName":"Peiling1 Yi","userId":"09881945369780419670"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["#Baseline"],"metadata":{"id":"1dvEJb-IGaol"}},{"cell_type":"code","source":["df_train=train\n","df_value=train\n","df_test=test\n","\n","dataset_train = Dataset.from_pandas(train)\n","dataset_val = Dataset.from_pandas(train[:10])\n","dataset_test = Dataset.from_pandas(test)\n","\n","dataset = DatasetDict({\n","    'train': dataset_train,\n","    'val': dataset_val,\n","    'test': dataset_test\n","  })\n","\n","model_new,tokenizer_new, training_args_new=define_model(model_name)\n","model_train,trainer,_=training(dataset,df_train,model_new,tokenizer_new, training_args_new)\n","make_predictions(model_train,df_test)\n","get_performance_metrics(df_test)"],"metadata":{"id":"4SyobsNGGgNa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ZCS"],"metadata":{"id":"GbrNbmTqMPGs"}},{"cell_type":"code","source":["\n","df_train=train_emotion_shuffle\n","df_value=train_emotion_shuffle[:10]\n","df_test=test\n","\n","dataset_train = Dataset.from_pandas(train_emotion_shuffle)\n","dataset_val = Dataset.from_pandas(train_emotion_shuffle[:10])\n","dataset_test = Dataset.from_pandas(test)\n","\n","dataset = DatasetDict({\n","    'train': dataset_train,\n","    'val': dataset_val,\n","    'test': dataset_test\n","  })\n","\n","model_new,tokenizer_new, training_args_new=define_model(model_name)\n","model_train_zsc,trainer,_=training(dataset,df_train,model_new,tokenizer_new, training_args_new)\n","make_predictions(model=model_train_zsc,df_test=df_test)\n","get_performance_metrics(df_test)"],"metadata":{"id":"BMjuCMcrMVi5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#FSC-based on ZSC for fine tunning"],"metadata":{"id":"oFjVXGG9G1cZ"}},{"cell_type":"code","source":["df_train=train\n","df_value=train\n","df_test=test\n","\n","dataset_train = Dataset.from_pandas(train)\n","dataset_val = Dataset.from_pandas(train[:10])\n","dataset_test = Dataset.from_pandas(test)\n","\n","\n","dataset = DatasetDict({\n","    'train': dataset_train,\n","    'val': dataset_val,\n","    'test': dataset_test\n","  })\n","\n","model_train_FCS,trainer_FCS,_=training(dataset=dataset,train_class=df_train,model=model_train_zsc,tokenizer=tokenizer_new,training_args=training_args_new)\n","make_predictions(model=model_train_FCS,df_test=df_test)\n","get_performance_metrics(df_test)"],"metadata":{"id":"2LoJC45_yyq8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZTHqTuPir_k"},"source":["## Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnCfi0Z3W567"},"outputs":[],"source":["metrics = train_result.metrics\n","max_train_samples = len(dataset_train)\n","metrics[\"train_samples\"] = min(max_train_samples, len(dataset_train))\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)\n","trainer.save_state()\n","trainer.save_model(\"saved_model\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}